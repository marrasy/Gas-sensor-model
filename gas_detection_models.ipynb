{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87221a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import copy\n",
    "import os\n",
    "import json\n",
    "# è®¾ç½®è®¾å¤‡\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52927585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== 1. Load and preprocess data =====================\n",
    "df = pd.read_excel('DataNT.xlsx')\n",
    "X = df.iloc[:, 6:].values         # Features: sensor1 to sensor30\n",
    "y = df.iloc[:, 1:5].values        # Labels: 6 gas presence (1/0)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88b8e074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== 2. Define models =====================\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout, num_layers):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        dim = input_dim\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.Linear(dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            dim = hidden_dim\n",
    "        layers.append(nn.Linear(dim, 4))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout, num_layers):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # (batch, seq=1, feature)\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        out = self.fc(h_n[-1])\n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_heads, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, 4)  # è¾“å‡º 6 ç»´\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # [32, 60] -> [32, 1, 60]\n",
    "        x = self.embedding(x)\n",
    "        x = self.encoder(x)\n",
    "        x = x.squeeze(1)  # [32, 1, hidden_dim] -> [32, hidden_dim]\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e1264a7-3077-4110-868c-78b353ebe463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== 3. Train & Evaluate Function =====================\n",
    "# è®­ç»ƒå’Œè¯„ä¼°å‡½æ•°\n",
    "def train_and_evaluate(model, train_loader, test_loader, epochs, lr):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    # è¯„ä¼°\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.sigmoid(outputs).cpu().numpy()\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    \n",
    "    y_pred = (np.vstack(all_preds) > 0.5).astype(int)\n",
    "    y_true = np.vstack(all_labels)\n",
    "\n",
    "    # å¤šæ ‡ç­¾ accuracyï¼ˆæŽ¨èï¼šmacro accuracyï¼‰\n",
    "    acc_per_label = (y_true == y_pred).mean(axis=0)\n",
    "    macro_acc = acc_per_label.mean()\n",
    "\n",
    "    return macro_acc, model, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "399d2224-e730-46f8-8472-f6287d3de110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== 4. Grid Search Settings =====================\n",
    "# ç½‘æ ¼æœç´¢\n",
    "param_grid = {\n",
    "    \"hidden_dim\": [48, 64, 128],\n",
    "    \"num_heads\": [2],\n",
    "    \"lr\": [0.001, 0.0005],\n",
    "    \"epochs\": [300,500],\n",
    "    \"dropout\": [0.1],\n",
    "    \"num_layers\": [3]\n",
    "}\n",
    "\n",
    "# è¿‡æ»¤ hidden_dim å’Œ num_heads å…¼å®¹æ€§\n",
    "valid_params = []\n",
    "for params in product(*param_grid.values()):\n",
    "    hidden_dim, num_heads, lr, epochs, dropout, num_layers = params\n",
    "    if hidden_dim % num_heads == 0:\n",
    "        valid_params.append(params)\n",
    "\n",
    "param_names = list(param_grid.keys())\n",
    "\n",
    "models = {\n",
    "    \"MLP\": MLP,\n",
    "    \"LSTM\": LSTMModel,\n",
    "    \"Transformer\": TransformerModel\n",
    "}\n",
    "\n",
    "results = []\n",
    "best_acc = -1\n",
    "best_config = None\n",
    "best_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c6e93eb-4f0b-4cc3-9ca4-998fb6ac7313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: MLP, Params: {'hidden_dim': 32, 'num_heads': 2, 'lr': 0.001, 'epochs': 300, 'dropout': 0.1, 'num_layers': 3}, Accuracy: 0.7396\n",
      "Model: MLP, Params: {'hidden_dim': 32, 'num_heads': 2, 'lr': 0.001, 'epochs': 300, 'dropout': 0.1, 'num_layers': 4}, Accuracy: 0.7083\n",
      "Model: MLP, Params: {'hidden_dim': 32, 'num_heads': 2, 'lr': 0.001, 'epochs': 500, 'dropout': 0.1, 'num_layers': 3}, Accuracy: 0.7708\n",
      "Model: MLP, Params: {'hidden_dim': 32, 'num_heads': 2, 'lr': 0.001, 'epochs': 500, 'dropout': 0.1, 'num_layers': 4}, Accuracy: 0.7396\n",
      "Model: MLP, Params: {'hidden_dim': 32, 'num_heads': 2, 'lr': 0.0005, 'epochs': 300, 'dropout': 0.1, 'num_layers': 3}, Accuracy: 0.7500\n",
      "Model: MLP, Params: {'hidden_dim': 32, 'num_heads': 2, 'lr': 0.0005, 'epochs': 300, 'dropout': 0.1, 'num_layers': 4}, Accuracy: 0.6875\n",
      "Model: MLP, Params: {'hidden_dim': 32, 'num_heads': 2, 'lr': 0.0005, 'epochs': 500, 'dropout': 0.1, 'num_layers': 3}, Accuracy: 0.7917\n",
      "Model: MLP, Params: {'hidden_dim': 32, 'num_heads': 2, 'lr': 0.0005, 'epochs': 500, 'dropout': 0.1, 'num_layers': 4}, Accuracy: 0.7188\n",
      "Model: MLP, Params: {'hidden_dim': 60, 'num_heads': 2, 'lr': 0.001, 'epochs': 300, 'dropout': 0.1, 'num_layers': 3}, Accuracy: 0.7604\n",
      "Model: MLP, Params: {'hidden_dim': 60, 'num_heads': 2, 'lr': 0.001, 'epochs': 300, 'dropout': 0.1, 'num_layers': 4}, Accuracy: 0.7188\n",
      "Model: MLP, Params: {'hidden_dim': 60, 'num_heads': 2, 'lr': 0.001, 'epochs': 500, 'dropout': 0.1, 'num_layers': 3}, Accuracy: 0.8229\n",
      "Model: MLP, Params: {'hidden_dim': 60, 'num_heads': 2, 'lr': 0.001, 'epochs': 500, 'dropout': 0.1, 'num_layers': 4}, Accuracy: 0.7292\n",
      "Model: MLP, Params: {'hidden_dim': 60, 'num_heads': 2, 'lr': 0.0005, 'epochs': 300, 'dropout': 0.1, 'num_layers': 3}, Accuracy: 0.7917\n",
      "Model: MLP, Params: {'hidden_dim': 60, 'num_heads': 2, 'lr': 0.0005, 'epochs': 300, 'dropout': 0.1, 'num_layers': 4}, Accuracy: 0.7500\n",
      "Model: MLP, Params: {'hidden_dim': 60, 'num_heads': 2, 'lr': 0.0005, 'epochs': 500, 'dropout': 0.1, 'num_layers': 3}, Accuracy: 0.7396\n",
      "Model: MLP, Params: {'hidden_dim': 60, 'num_heads': 2, 'lr': 0.0005, 'epochs': 500, 'dropout': 0.1, 'num_layers': 4}, Accuracy: 0.7292\n",
      "Model: MLP, Params: {'hidden_dim': 64, 'num_heads': 2, 'lr': 0.001, 'epochs': 300, 'dropout': 0.1, 'num_layers': 3}, Accuracy: 0.7500\n",
      "Model: MLP, Params: {'hidden_dim': 64, 'num_heads': 2, 'lr': 0.001, 'epochs': 300, 'dropout': 0.1, 'num_layers': 4}, Accuracy: 0.7396\n",
      "Model: MLP, Params: {'hidden_dim': 64, 'num_heads': 2, 'lr': 0.001, 'epochs': 500, 'dropout': 0.1, 'num_layers': 3}, Accuracy: 0.7396\n",
      "Model: MLP, Params: {'hidden_dim': 64, 'num_heads': 2, 'lr': 0.001, 'epochs': 500, 'dropout': 0.1, 'num_layers': 4}, Accuracy: 0.8229\n",
      "Model: MLP, Params: {'hidden_dim': 64, 'num_heads': 2, 'lr': 0.0005, 'epochs': 300, 'dropout': 0.1, 'num_layers': 3}, Accuracy: 0.7292\n",
      "Model: MLP, Params: {'hidden_dim': 64, 'num_heads': 2, 'lr': 0.0005, 'epochs': 300, 'dropout': 0.1, 'num_layers': 4}, Accuracy: 0.7604\n",
      "Model: MLP, Params: {'hidden_dim': 64, 'num_heads': 2, 'lr': 0.0005, 'epochs': 500, 'dropout': 0.1, 'num_layers': 3}, Accuracy: 0.7917\n",
      "Model: MLP, Params: {'hidden_dim': 64, 'num_heads': 2, 'lr': 0.0005, 'epochs': 500, 'dropout': 0.1, 'num_layers': 4}, Accuracy: 0.7604\n",
      "Model: MLP, Params: {'hidden_dim': 256, 'num_heads': 2, 'lr': 0.001, 'epochs': 300, 'dropout': 0.1, 'num_layers': 3}, Accuracy: 0.7396\n",
      "Model: MLP, Params: {'hidden_dim': 256, 'num_heads': 2, 'lr': 0.001, 'epochs': 300, 'dropout': 0.1, 'num_layers': 4}, Accuracy: 0.7083\n",
      "Model: MLP, Params: {'hidden_dim': 256, 'num_heads': 2, 'lr': 0.001, 'epochs': 500, 'dropout': 0.1, 'num_layers': 3}, Accuracy: 0.7604\n",
      "Model: MLP, Params: {'hidden_dim': 256, 'num_heads': 2, 'lr': 0.001, 'epochs': 500, 'dropout': 0.1, 'num_layers': 4}, Accuracy: 0.7604\n",
      "Model: MLP, Params: {'hidden_dim': 256, 'num_heads': 2, 'lr': 0.0005, 'epochs': 300, 'dropout': 0.1, 'num_layers': 3}, Accuracy: 0.7500\n",
      "Model: MLP, Params: {'hidden_dim': 256, 'num_heads': 2, 'lr': 0.0005, 'epochs': 300, 'dropout': 0.1, 'num_layers': 4}, Accuracy: 0.7292\n",
      "Model: MLP, Params: {'hidden_dim': 256, 'num_heads': 2, 'lr': 0.0005, 'epochs': 500, 'dropout': 0.1, 'num_layers': 3}, Accuracy: 0.7292\n",
      "Model: MLP, Params: {'hidden_dim': 256, 'num_heads': 2, 'lr': 0.0005, 'epochs': 500, 'dropout': 0.1, 'num_layers': 4}, Accuracy: 0.7500\n"
     ]
    }
   ],
   "source": [
    "for model_name, model_class in models.items():\n",
    "    for param_values in valid_params:\n",
    "        params = dict(zip(param_names, param_values))\n",
    "\n",
    "        if model_name == \"Transformer\":\n",
    "            model = model_class(\n",
    "                input_dim=60,\n",
    "                hidden_dim=params[\"hidden_dim\"],\n",
    "                num_heads=params[\"num_heads\"],\n",
    "                num_layers=params[\"num_layers\"],\n",
    "                dropout=params[\"dropout\"]\n",
    "            )\n",
    "        elif model_name == \"LSTM\":\n",
    "            model = model_class(\n",
    "                input_dim=60,\n",
    "                hidden_dim=params[\"hidden_dim\"],\n",
    "                num_layers=params[\"num_layers\"],\n",
    "                dropout=params[\"dropout\"]\n",
    "            )\n",
    "        else:  # MLP\n",
    "            model = model_class(\n",
    "                input_dim=60,\n",
    "                hidden_dim=params[\"hidden_dim\"],\n",
    "                num_layers=params[\"num_layers\"],\n",
    "                dropout=params[\"dropout\"]\n",
    "            )\n",
    "\n",
    "        acc, trained_model, y_pred = train_and_evaluate(\n",
    "            model, train_loader, test_loader, \n",
    "            params[\"epochs\"], params[\"lr\"]\n",
    "        )\n",
    "\n",
    "        results.append((model_name, params, acc))\n",
    "        print(f\"Model: {model_name}, Params: {params}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_config = (model_name, params)\n",
    "            best_model = copy.deepcopy(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81776e4e-107a-40b5-ae85-bb66cde09b53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ecbb3d3-d221-472a-afd3-2338f50e1975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8229166666666666\n",
      "âœ… æ¨¡åž‹å’Œé…ç½®å·²ä¿å­˜ï¼\n"
     ]
    }
   ],
   "source": [
    "# åˆ›å»ºä¿å­˜ç›®å½•\n",
    "os.makedirs(\"model_output\", exist_ok=True)\n",
    "\n",
    "# ä¿å­˜æ¨¡åž‹æƒé‡\n",
    "torch.save(best_model.state_dict(), \"model_output/best_model.pth\")\n",
    "\n",
    "# ä¿å­˜é…ç½®ä¿¡æ¯ï¼ˆæ¨¡åž‹å + å‚æ•° + F1 åˆ†æ•°ï¼‰\n",
    "best_model_info = {\n",
    "    \"model_name\": best_config[0],\n",
    "    \"params\": best_config[1],\n",
    "    \"Acc_score\": best_acc\n",
    "}\n",
    "\n",
    "with open(\"model_output/best_model_config.json\", \"w\") as f:\n",
    "    json.dump(best_model_info, f, indent=4)\n",
    "print(best_acc)\n",
    "print(\"âœ… æ¨¡åž‹å’Œé…ç½®å·²ä¿å­˜ï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b534a330-8cc5-47a5-9eed-214f2708e872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ¨¡åž‹åŠ è½½å®Œæ¯•ï¼çŽ°åœ¨å¯ä»¥ç”¨ model(inputs) è¿›è¡Œé¢„æµ‹å•¦~\n",
      "\n",
      "ðŸ“Š Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.83      0.91        12\n",
      "           1       0.54      0.88      0.67         8\n",
      "           2       0.83      0.91      0.87        11\n",
      "           3       0.77      0.83      0.80        12\n",
      "\n",
      "   micro avg       0.77      0.86      0.81        43\n",
      "   macro avg       0.79      0.86      0.81        43\n",
      "weighted avg       0.81      0.86      0.82        43\n",
      " samples avg       0.77      0.81      0.76        43\n",
      "\n",
      "\n",
      "ðŸŒ€ Confusion Matrix:\n",
      "[[[12  0]\n",
      "  [ 2 10]]\n",
      "\n",
      " [[10  6]\n",
      "  [ 1  7]]\n",
      "\n",
      " [[11  2]\n",
      "  [ 1 10]]\n",
      "\n",
      " [[ 9  3]\n",
      "  [ 2 10]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\y'j'm\\AppData\\Local\\Temp\\ipykernel_14852\\363884526.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model_output/best_model_4.0.1.pth\"))\n",
      "D:\\anacoda\\envs\\bishe\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "# ==== è¯»å–ä¿å­˜çš„æ¨¡åž‹é…ç½®ä¿¡æ¯ ====\n",
    "with open(\"model_output/best_model_config_4.0.1.json.\", \"r\") as f:\n",
    "    model_info = json.load(f)\n",
    "\n",
    "model_name = model_info[\"model_name\"]\n",
    "params = model_info[\"params\"]\n",
    "\n",
    "# ==== é‡æ–°æž„å»ºæ¨¡åž‹ç»“æž„ ====\n",
    "if model_name == \"MLP\":\n",
    "    model = MLP(input_dim=60, hidden_dim=params[\"hidden_dim\"], dropout=params[\"dropout\"], num_layers=params[\"num_layers\"])\n",
    "elif model_name == \"LSTM\":\n",
    "    model = LSTMModel(input_dim=60, hidden_dim=params[\"hidden_dim\"], dropout=params[\"dropout\"], num_layers=params[\"num_layers\"])\n",
    "elif model_name == \"Transformer\":\n",
    "    model = TransformerModel(input_dim=60, hidden_dim=params[\"hidden_dim\"], dropout=params[\"dropout\"], num_layers=params[\"num_layers\"])\n",
    "else:\n",
    "    raise ValueError(\"Unknown model name!\")\n",
    "\n",
    "# ==== åŠ è½½æ¨¡åž‹å‚æ•° ====\n",
    "model.load_state_dict(torch.load(\"model_output/best_model_4.0.1.pth\"))\n",
    "model.eval()\n",
    "\n",
    "print(\"âœ… æ¨¡åž‹åŠ è½½å®Œæ¯•ï¼çŽ°åœ¨å¯ä»¥ç”¨ model(inputs) è¿›è¡Œé¢„æµ‹å•¦~\")\n",
    "#å…·ä½“å¦‚ä½•é¢„æµ‹\n",
    "with torch.no_grad():\n",
    "    outputs = model(torch.tensor(X_test, dtype=torch.float32).to(device))\n",
    "    preds = (torch.sigmoid(outputs) > 0.5).int().cpu().numpy()\n",
    "\n",
    "    print(\"\\nðŸ“Š Classification Report:\")\n",
    "    print(classification_report(y_test, preds))\n",
    "\n",
    "    # ==== æ··æ·†çŸ©é˜µ ====\n",
    "    cm = multilabel_confusion_matrix(y_test, preds)\n",
    "    print(\"\\nðŸŒ€ Confusion Matrix:\")\n",
    "    print(cm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
