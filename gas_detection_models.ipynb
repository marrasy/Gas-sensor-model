{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87221a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import copy\n",
    "import os\n",
    "import json\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52927585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== 1. Load and preprocess data =====================\n",
    "df = pd.read_excel('DataNT.xlsx')\n",
    "X = df.iloc[:, 6:].values         # Features: sensor1 to sensor30\n",
    "y = df.iloc[:, 1:5].values        # Labels: 6 gas presence (1/0)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88b8e074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== 2. Define models =====================\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout, num_layers):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        dim = input_dim\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.Linear(dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            dim = hidden_dim\n",
    "        layers.append(nn.Linear(dim, 4))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout, num_layers):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # (batch, seq=1, feature)\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        out = self.fc(h_n[-1])\n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_heads, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, 4)  # 输出 6 维\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # [32, 60] -> [32, 1, 60]\n",
    "        x = self.embedding(x)\n",
    "        x = self.encoder(x)\n",
    "        x = x.squeeze(1)  # [32, 1, hidden_dim] -> [32, hidden_dim]\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e1264a7-3077-4110-868c-78b353ebe463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== 3. Train & Evaluate Function =====================\n",
    "# 训练和评估函数\n",
    "def train_and_evaluate(model, train_loader, test_loader, epochs, lr):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    # 评估\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.sigmoid(outputs).cpu().numpy()\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    \n",
    "    y_pred = (np.vstack(all_preds) > 0.5).astype(int)\n",
    "    y_true = np.vstack(all_labels)\n",
    "\n",
    "    # 多标签 accuracy（推荐：macro accuracy）\n",
    "    acc_per_label = (y_true == y_pred).mean(axis=0)\n",
    "    macro_acc = acc_per_label.mean()\n",
    "\n",
    "    return macro_acc, model, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "399d2224-e730-46f8-8472-f6287d3de110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== 4. Grid Search Settings =====================\n",
    "# 网格搜索\n",
    "param_grid = {\n",
    "    \"hidden_dim\": [48, 64, 128],\n",
    "    \"num_heads\": [2],\n",
    "    \"lr\": [0.001, 0.0005],\n",
    "    \"epochs\": [300,500],\n",
    "    \"dropout\": [0.1],\n",
    "    \"num_layers\": [3]\n",
    "}\n",
    "\n",
    "# 过滤 hidden_dim 和 num_heads 兼容性\n",
    "valid_params = []\n",
    "for params in product(*param_grid.values()):\n",
    "    hidden_dim, num_heads, lr, epochs, dropout, num_layers = params\n",
    "    if hidden_dim % num_heads == 0:\n",
    "        valid_params.append(params)\n",
    "\n",
    "param_names = list(param_grid.keys())\n",
    "\n",
    "models = {\n",
    "    \"MLP\": MLP,\n",
    "    \"LSTM\": LSTMModel,\n",
    "    \"Transformer\": TransformerModel\n",
    "}\n",
    "\n",
    "results = []\n",
    "best_acc = -1\n",
    "best_config = None\n",
    "best_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c6e93eb-4f0b-4cc3-9ca4-998fb6ac7313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: MLP, Params: {'hidden_dim': 32, 'num_heads': 2, 'lr': 0.001, 'epochs': 300, 'dropout': 0.1, 'num_layers': 3}, Accuracy: 0.7396\n",
      "Model: MLP, Params: {'hidden_dim': 32, 'num_heads': 2, 'lr': 0.001, 'epochs': 300, 'dropout': 0.1, 'num_layers': 4}, Accuracy: 0.7083\n",
      "Model: MLP, Params: {'hidden_dim': 32, 'num_heads': 2, 'lr': 0.001, 'epochs': 500, 'dropout': 0.1, 'num_layers': 3}, Accuracy: 0.7708\n",
      "Model: MLP, Params: {'hidden_dim': 32, 'num_heads': 2, 'lr': 0.001, 'epochs': 500, 'dropout': 0.1, 'num_layers': 4}, Accuracy: 0.7396\n",
      "Model: MLP, Params: {'hidden_dim': 32, 'num_heads': 2, 'lr': 0.0005, 'epochs': 300, 'dropout': 0.1, 'num_layers': 3}, Accuracy: 0.7500\n",
      "Model: MLP, Params: {'hidden_dim': 32, 'num_heads': 2, 'lr': 0.0005, 'epochs': 300, 'dropout': 0.1, 'num_layers': 4}, Accuracy: 0.6875\n",
      "Model: MLP, Params: {'hidden_dim': 32, 'num_heads': 2, 'lr': 0.0005, 'epochs': 500, 'dropout': 0.1, 'num_layers': 3}, Accuracy: 0.7917\n",
      "Model: MLP, Params: {'hidden_dim': 32, 'num_heads': 2, 'lr': 0.0005, 'epochs': 500, 'dropout': 0.1, 'num_layers': 4}, Accuracy: 0.7188\n",
      "Model: MLP, Params: {'hidden_dim': 60, 'num_heads': 2, 'lr': 0.001, 'epochs': 300, 'dropout': 0.1, 'num_layers': 3}, Accuracy: 0.7604\n",
      "Model: MLP, Params: {'hidden_dim': 60, 'num_heads': 2, 'lr': 0.001, 'epochs': 300, 'dropout': 0.1, 'num_layers': 4}, Accuracy: 0.7188\n",
      "Model: MLP, Params: {'hidden_dim': 60, 'num_heads': 2, 'lr': 0.001, 'epochs': 500, 'dropout': 0.1, 'num_layers': 3}, Accuracy: 0.8229\n",
      "Model: MLP, Params: {'hidden_dim': 60, 'num_heads': 2, 'lr': 0.001, 'epochs': 500, 'dropout': 0.1, 'num_layers': 4}, Accuracy: 0.7292\n",
      "Model: MLP, Params: {'hidden_dim': 60, 'num_heads': 2, 'lr': 0.0005, 'epochs': 300, 'dropout': 0.1, 'num_layers': 3}, Accuracy: 0.7917\n",
      "Model: MLP, Params: {'hidden_dim': 60, 'num_heads': 2, 'lr': 0.0005, 'epochs': 300, 'dropout': 0.1, 'num_layers': 4}, Accuracy: 0.7500\n",
      "Model: MLP, Params: {'hidden_dim': 60, 'num_heads': 2, 'lr': 0.0005, 'epochs': 500, 'dropout': 0.1, 'num_layers': 3}, Accuracy: 0.7396\n",
      "Model: MLP, Params: {'hidden_dim': 60, 'num_heads': 2, 'lr': 0.0005, 'epochs': 500, 'dropout': 0.1, 'num_layers': 4}, Accuracy: 0.7292\n",
      "Model: MLP, Params: {'hidden_dim': 64, 'num_heads': 2, 'lr': 0.001, 'epochs': 300, 'dropout': 0.1, 'num_layers': 3}, Accuracy: 0.7500\n",
      "Model: MLP, Params: {'hidden_dim': 64, 'num_heads': 2, 'lr': 0.001, 'epochs': 300, 'dropout': 0.1, 'num_layers': 4}, Accuracy: 0.7396\n",
      "Model: MLP, Params: {'hidden_dim': 64, 'num_heads': 2, 'lr': 0.001, 'epochs': 500, 'dropout': 0.1, 'num_layers': 3}, Accuracy: 0.7396\n",
      "Model: MLP, Params: {'hidden_dim': 64, 'num_heads': 2, 'lr': 0.001, 'epochs': 500, 'dropout': 0.1, 'num_layers': 4}, Accuracy: 0.8229\n",
      "Model: MLP, Params: {'hidden_dim': 64, 'num_heads': 2, 'lr': 0.0005, 'epochs': 300, 'dropout': 0.1, 'num_layers': 3}, Accuracy: 0.7292\n",
      "Model: MLP, Params: {'hidden_dim': 64, 'num_heads': 2, 'lr': 0.0005, 'epochs': 300, 'dropout': 0.1, 'num_layers': 4}, Accuracy: 0.7604\n",
      "Model: MLP, Params: {'hidden_dim': 64, 'num_heads': 2, 'lr': 0.0005, 'epochs': 500, 'dropout': 0.1, 'num_layers': 3}, Accuracy: 0.7917\n",
      "Model: MLP, Params: {'hidden_dim': 64, 'num_heads': 2, 'lr': 0.0005, 'epochs': 500, 'dropout': 0.1, 'num_layers': 4}, Accuracy: 0.7604\n",
      "Model: MLP, Params: {'hidden_dim': 256, 'num_heads': 2, 'lr': 0.001, 'epochs': 300, 'dropout': 0.1, 'num_layers': 3}, Accuracy: 0.7396\n",
      "Model: MLP, Params: {'hidden_dim': 256, 'num_heads': 2, 'lr': 0.001, 'epochs': 300, 'dropout': 0.1, 'num_layers': 4}, Accuracy: 0.7083\n",
      "Model: MLP, Params: {'hidden_dim': 256, 'num_heads': 2, 'lr': 0.001, 'epochs': 500, 'dropout': 0.1, 'num_layers': 3}, Accuracy: 0.7604\n",
      "Model: MLP, Params: {'hidden_dim': 256, 'num_heads': 2, 'lr': 0.001, 'epochs': 500, 'dropout': 0.1, 'num_layers': 4}, Accuracy: 0.7604\n",
      "Model: MLP, Params: {'hidden_dim': 256, 'num_heads': 2, 'lr': 0.0005, 'epochs': 300, 'dropout': 0.1, 'num_layers': 3}, Accuracy: 0.7500\n",
      "Model: MLP, Params: {'hidden_dim': 256, 'num_heads': 2, 'lr': 0.0005, 'epochs': 300, 'dropout': 0.1, 'num_layers': 4}, Accuracy: 0.7292\n",
      "Model: MLP, Params: {'hidden_dim': 256, 'num_heads': 2, 'lr': 0.0005, 'epochs': 500, 'dropout': 0.1, 'num_layers': 3}, Accuracy: 0.7292\n",
      "Model: MLP, Params: {'hidden_dim': 256, 'num_heads': 2, 'lr': 0.0005, 'epochs': 500, 'dropout': 0.1, 'num_layers': 4}, Accuracy: 0.7500\n"
     ]
    }
   ],
   "source": [
    "for model_name, model_class in models.items():\n",
    "    for param_values in valid_params:\n",
    "        params = dict(zip(param_names, param_values))\n",
    "\n",
    "        if model_name == \"Transformer\":\n",
    "            model = model_class(\n",
    "                input_dim=60,\n",
    "                hidden_dim=params[\"hidden_dim\"],\n",
    "                num_heads=params[\"num_heads\"],\n",
    "                num_layers=params[\"num_layers\"],\n",
    "                dropout=params[\"dropout\"]\n",
    "            )\n",
    "        elif model_name == \"LSTM\":\n",
    "            model = model_class(\n",
    "                input_dim=60,\n",
    "                hidden_dim=params[\"hidden_dim\"],\n",
    "                num_layers=params[\"num_layers\"],\n",
    "                dropout=params[\"dropout\"]\n",
    "            )\n",
    "        else:  # MLP\n",
    "            model = model_class(\n",
    "                input_dim=60,\n",
    "                hidden_dim=params[\"hidden_dim\"],\n",
    "                num_layers=params[\"num_layers\"],\n",
    "                dropout=params[\"dropout\"]\n",
    "            )\n",
    "\n",
    "        acc, trained_model, y_pred = train_and_evaluate(\n",
    "            model, train_loader, test_loader, \n",
    "            params[\"epochs\"], params[\"lr\"]\n",
    "        )\n",
    "\n",
    "        results.append((model_name, params, acc))\n",
    "        print(f\"Model: {model_name}, Params: {params}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_config = (model_name, params)\n",
    "            best_model = copy.deepcopy(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81776e4e-107a-40b5-ae85-bb66cde09b53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ecbb3d3-d221-472a-afd3-2338f50e1975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8229166666666666\n",
      "✅ 模型和配置已保存！\n"
     ]
    }
   ],
   "source": [
    "# 创建保存目录\n",
    "os.makedirs(\"model_output\", exist_ok=True)\n",
    "\n",
    "# 保存模型权重\n",
    "torch.save(best_model.state_dict(), \"model_output/best_model.pth\")\n",
    "\n",
    "# 保存配置信息（模型名 + 参数 + F1 分数）\n",
    "best_model_info = {\n",
    "    \"model_name\": best_config[0],\n",
    "    \"params\": best_config[1],\n",
    "    \"Acc_score\": best_acc\n",
    "}\n",
    "\n",
    "with open(\"model_output/best_model_config.json\", \"w\") as f:\n",
    "    json.dump(best_model_info, f, indent=4)\n",
    "print(best_acc)\n",
    "print(\"✅ 模型和配置已保存！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b534a330-8cc5-47a5-9eed-214f2708e872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 模型加载完毕！现在可以用 model(inputs) 进行预测啦~\n",
      "\n",
      "📊 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.83      0.91        12\n",
      "           1       0.54      0.88      0.67         8\n",
      "           2       0.83      0.91      0.87        11\n",
      "           3       0.77      0.83      0.80        12\n",
      "\n",
      "   micro avg       0.77      0.86      0.81        43\n",
      "   macro avg       0.79      0.86      0.81        43\n",
      "weighted avg       0.81      0.86      0.82        43\n",
      " samples avg       0.77      0.81      0.76        43\n",
      "\n",
      "\n",
      "🌀 Confusion Matrix:\n",
      "[[[12  0]\n",
      "  [ 2 10]]\n",
      "\n",
      " [[10  6]\n",
      "  [ 1  7]]\n",
      "\n",
      " [[11  2]\n",
      "  [ 1 10]]\n",
      "\n",
      " [[ 9  3]\n",
      "  [ 2 10]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\y'j'm\\AppData\\Local\\Temp\\ipykernel_14852\\363884526.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model_output/best_model_4.0.1.pth\"))\n",
      "D:\\anacoda\\envs\\bishe\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "# ==== 读取保存的模型配置信息 ====\n",
    "with open(\"model_output/best_model_config_4.0.1.json.\", \"r\") as f:\n",
    "    model_info = json.load(f)\n",
    "\n",
    "model_name = model_info[\"model_name\"]\n",
    "params = model_info[\"params\"]\n",
    "\n",
    "# ==== 重新构建模型结构 ====\n",
    "if model_name == \"MLP\":\n",
    "    model = MLP(input_dim=60, hidden_dim=params[\"hidden_dim\"], dropout=params[\"dropout\"], num_layers=params[\"num_layers\"])\n",
    "elif model_name == \"LSTM\":\n",
    "    model = LSTMModel(input_dim=60, hidden_dim=params[\"hidden_dim\"], dropout=params[\"dropout\"], num_layers=params[\"num_layers\"])\n",
    "elif model_name == \"Transformer\":\n",
    "    model = TransformerModel(input_dim=60, hidden_dim=params[\"hidden_dim\"], dropout=params[\"dropout\"], num_layers=params[\"num_layers\"])\n",
    "else:\n",
    "    raise ValueError(\"Unknown model name!\")\n",
    "\n",
    "# ==== 加载模型参数 ====\n",
    "model.load_state_dict(torch.load(\"model_output/best_model_4.0.1.pth\"))\n",
    "model.eval()\n",
    "\n",
    "print(\"✅ 模型加载完毕！现在可以用 model(inputs) 进行预测啦~\")\n",
    "#具体如何预测\n",
    "with torch.no_grad():\n",
    "    outputs = model(torch.tensor(X_test, dtype=torch.float32).to(device))\n",
    "    preds = (torch.sigmoid(outputs) > 0.5).int().cpu().numpy()\n",
    "\n",
    "    print(\"\\n📊 Classification Report:\")\n",
    "    print(classification_report(y_test, preds))\n",
    "\n",
    "    # ==== 混淆矩阵 ====\n",
    "    cm = multilabel_confusion_matrix(y_test, preds)\n",
    "    print(\"\\n🌀 Confusion Matrix:\")\n",
    "    print(cm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
